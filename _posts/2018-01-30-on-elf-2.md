---
layout: post
title:  "On ELF, Part 2"
date:   2018-01-30 21:00:00
---

## Abstract

The Executable and Linkable Format, ELF, is too complex.
I firmly feel it was created without critically thinking about the ramifications it would have on future tool chains.
In this article, I show how ELF-like features can be safely retrofitted onto contemporary executable formats.
Hopefully, future executable format authors will reconsider their needs more critically in the future before committing to something as complicated as ELF.

## Basic Principles

Before writing this installment,
I had a realization.
One of the big mistakes from Unix,
besides the X Window System,
was the `ld` linker.
At the time, it must have seemed like a great idea!
Put yourself in the original authors' shoes:
why should the kernel spend so much effort relocating a binary
when that image will just appear at the same place in every process?
So, why not just relocate any linked executable at that point *before* the kernel ever gets a chance to see it?
The kernel can be made much simpler
(just read in an opaque binary blob, then call a specified address),
and as a nice side-effect,
it's faster to run loaded programs as well.

But, as we've seen in the previous article,
this can open a can of worms from the security point of view
as well as constrain new features,
such as the new hotness at about this time, component-oriented programming
(what eventually led to CORBA and DCOM).
It turns out that program relocation at load-time is a *good* thing.<sup>[1](#opposingView)</sup>
The design of ELF, in part,
reflects the realization that mistakes were made in the past,
and AT&T wanted to both move forward with new features while preserving at least some of their initial investment.
(Aside: I, personally, have mixed feelings about this;
but, this article is not about those specific feelings.)

So, *why* is `ld` such a big mistake?
Bluntly, it's a tool which, like ELF, fails to adhere to the Unix philosophy of
doing one thing well.
See, `ld` is responsible for at least two services:

* Linking multiple compiler artifacts into a single artifact for later re-use in the construction of software, and,
* Producing a loadable binary from one or more of these re-usable artifacts.

While these two concepts are clearly *related*,
they are most definitely not *the same thing*.
We see that the
[Plan 9 compiler toolchain](http://doc.cat-v.org/bell_labs/new_c_compilers/new_c_compiler.pdf)
partially remedies this past oversight
by keeping software in a binary representation of an assembly language listing
for as long as possible before the very last step of producing the final binary executable.

Basically, there are two principles involved.

**Principle of Linking.**
A linker's job is to coalesce, merge, and perhaps even sort *sections* of equivalent *type*.
If it helps you, think of it as a merge-sort for related code.
A typical program might have hundreds of sections,
which a linker might reduce to a smaller, perhaps still quite numerous, quantity.
Ultimately, by the time you yield a final executable,
it's been massaged down to a much smaller set
(typically, text, data, and BSS).

**Principle of Loading.**
A loader's job is to unmarshall the concrete program representation in memory from an off-line representation.
This *prepares* a program for execution.
In the case of dynamic linking,
a loader *may* perform some basic relocations,
but *it never merges sections of comparable types.*
It just lays them out in the address space somehow.
This can be seen on any Linux installation by looking at a process' memory map layout
(`sudo cat /proc/$PID/maps`,
which will show each dynamically loaded module's text, data, BSS, and stack segments).
I'm sure BSD and Plan 9 have similarly accessible methods of showing this information.

Where `ld` goes wrong, then,
is that it attempts to both *link* and *load* in one step.
The limitations of this approach do not become visible as long as

* Your runtime environment is isolated via a page-capable MMU, and,
* You have no need whatsoever for dynamic linking.

Violate any of these assumptions, and
`ld`'s approach breaks down hard.
This is why `ld` requires so many horrible-looking arguments when linking code together:
one set of options intends to put a choke-hold on `ld` from building an executable or shared object
when all you want is a relocatable module,
while another set is often to do just the opposite.

I mention this only because
I want this article to focus exclusively on *program loading.*
That is, nothing I write in this article is intended to relate to *linking* at all.
The formats below may or may not make suitable targets for linkers.
In fact, there's a great chance that they won't at all.
Plan 9 accepts this.  There's no reason why the progeny of Unix can't either.

So, with the full understanding that we're talking *exclusively* about *loading* programs,
let us now tear ELF a new hole in its address space by illustrating viable alternatives to it.

## ELF versus GEMDOS PRG Format

In the previous article,
I had stated that the `a.out` format was
one of the earliest and most portable of executable formats around.
To illustrate this point,
let's compare [Unix's original `a.out` format](https://www.bell-labs.com/usr/dmr/www/man51.pdf) to
an OS where you might not have expected it to show up:
[Atari TOS and GEMDOS](http://cd.textfiles.com/ataricompendium/BOOK/HTML/CHAP2.HTM#processes),
the two components that make up the Atari ST/TT's operating system.

**PDP-11 Unix**

    struct aout_unix {
        short   a_magic;    // 0x0085
        short   a_textsz;   // Size of .text segment
        short   a_symsz;    // Size of symbol tables
        short   a_relocsz;  // Size of fixups
        short   a_datasz;   // Size of .data segment
        short   a_padding;
    };

**Atari TOS**

    struct aout_tos {
        short   PRG_magic;  // 0x601A
        long    PRG_tsize;  // Size of .text segment
        long    PRG_dsize;  // Size of .data segment
        long    PRG_bsize;  // Size of .bss segment
        long    PRG_ssize;  // Size of symbol tables
        long    PRG_res1;   // reserved
        long    PRGFLAGS;   // Loader flags to TOS
        long    ABSFLAG;    // More flags
        // ... text, data, and othe segments follow
        long    PRG_fixup;  // Offset to fixups
        // ... other content
        // ... fixups
    };

Some observations:

* The respective fields appear in different places depending on which header is used.  But, that's OK; ELF 32-bit and 64-bit structures also have fields which move about to support different processor alignment restrictions.
* TOS supports an explicit `.bss` segment, while the original Unix loader did not.  I'm guessing BSS was just a pre-zeroed bunch of bytes tacked onto the end of `.data`, and accounted for in `a_datasz`.
* Interestingly, both the PDP-11 and the 68000 formats supported fixups to resolve code and data references.  This suggests there was a time when Unix did not run with a page-based MMU.

That's it.  The `a.out` format is surprisingly versatile considering its simplicity.
It supported TOS applications in a single address space,
it supported MultiTOS applications in a single address space,
and it later supported MultiTOS applications in *separate* address spaces as well.
Clearly, we see that `a.out` is quite adept
at handling both DOS-like and Unix-like environments with equal facility.

Note that some details differ between the two `a.out` headers.
That's perfectly OK.
Exact layout doesn't matter,
what matters are the concepts supported.
It gives the loader just enough information to allocate a chunk of memory for code,
a chunk of memory for data and BSS,
to locate the relevant data in the files and load them into the allocated memory,
and then apply fixups to resolve broken references.
The `a.out` binaries were self-contained and self-consistent otherwise:
it was not possible to legally create an `a.out` file
which referred to an undefined symbol.
Inter-section relocations are resolved by the linker prior to emitting the final `a.out`,
so that (in most cases) they used PC-relative or register-relative addressing.
Absolute references (e.g., with JSR or JMP instructions) were resolved using the
bundled fixups.

In the previous article,
I'd stated that `a.out` was intended to be loaded/mapped blindly as an opaque blob.
This is especially evident in the original Unix header,
as the "magic" field, used to identify executables from other file types,
is *literally* a PDP-11 machine language instruction that *jumps over the header.*

### Extending to Support Dynamic Linking

OK, so now that we've seen how trivial `a.out` is,
how do we extend it to support dynamic linking?
Pretend, for a moment, that you were in charge of adding
dynamic linking to MultiTOS for its next release.
How would you retrofit this loader format to do so?
Here is how *I* would handle the task.

### Required Extensions

The first task would be to identify what additional support
is required to handle the job.

First and foremost, a statically linked binary
is expected to be self-contained.
Dynamic linking throws that assumption out the door;
therefore, we need a way for a module to identify its dependencies.
So, we need to introduce a segment that lists library dependencies.

Second, we observe that dynamically linked modules
may require symbols defined in the executable.
It turns out `a.out` has us covered here,
as both PDP-11 and TOS versions of the file support a symbol table explicitly.
Convenient!
So out-bound definitions are taken care of;
however, *in-bound* relocations are not.
There are two ways to handle this:
first, we can extend the existing symbol table definition
to support both symbol definitions *and* symbol references,
or we can introduce a separate symbol table segment just for in-bound references.
Just to make things harder and for the sake of illustration,
we'll assume the latter.
In the real-world, I'd probably shoot for the former.

To support in-bound symbol relocations,
we will also need to extend our relocation records.
Atari TOS uses a very simplistic model:
every N bytes, add the base address of the appropriate segment to the 32-bit word at that current location.
Again, the assumption is that references are self-consistent/self-contained,
so this is fine if all you're doing is relocating a microcosm of code.
For dynamic linking, however,
we need to know not only where to perform the relocations,
but also against what symbol.
For this reason,
I would replace individual bytes in the relocation segment
with 32-bit words, with a layout along these lines:

    +---+-----------+
    | D |     d     |  Type 1
    +---+-----------+

    +---+-----------+
    | D |     S     |  Type 2
    +---+-----------+

The rules are as follows:

* If D=255, then we have a type-1 relocation record, which basically says, "The next relocation record is going apply at offset d in the file."  This limits your binary size to 16MB as defined (24-bit d field), maybe 64MB if you can guarantee that all 32-bit offsets are 32-bit aligned.  However, I doubt this would ever be a problem.
* If D<255, then we have a type-2 relocation record.  This says to the loader, "Fix up the current 32-bit word with the value of the S-th symbol in the symbol table, then advance D bytes in the program image."

In both cases,
we know if we're making a code or data segment reference based on the relative offset we're patching in the `a.out` file.

## ELF versus AmigaDOS Hunk Format
## Conclusion

t.b.d.

----
<a name="opposingView">1</a>:
For a quite compelling argument against this point of view,
please read this collection of quotes against
[dynamic linking in general,](http://harmful.cat-v.org/software/dynamic-linking/)
and a uniquely ELF-related concept of
[*versioned symbols*.](http://harmful.cat-v.org/software/dynamic-linking/versioned-symbols)

